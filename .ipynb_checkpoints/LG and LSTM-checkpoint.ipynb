{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252ec033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('fake reviews dataset.csv')\n",
    "df['label'] = df['label'].map({'CG': 1, 'OR': 0})\n",
    "\n",
    "# Define the feature (X) and target (y) variables\n",
    "X = df['text_']  # The review text\n",
    "y = df['label']  # The labels\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data into TF-IDF features for Logistic Regression\n",
    "vectorizer = TfidfVectorizer(max_features=3000)  # Reduced features\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=500)  # Reduced iterations\n",
    "logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Calculate accuracy for Logistic Regression\n",
    "logistic_pred = logistic_model.predict(X_test_tfidf)\n",
    "logistic_accuracy = accuracy_score(y_test, logistic_pred)\n",
    "print(f\"Logistic Regression Model Accuracy: {logistic_accuracy:.2f}\")\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "max_words = 3000  # Reduced words\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Try to load the LSTM model if it was previously saved\n",
    "try:\n",
    "    lstm_model = load_model('lstm_model.h5')\n",
    "    print(\"LSTM model loaded from disk.\")\n",
    "except:\n",
    "    # Build and train the LSTM model if it was not found\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(Embedding(max_words, 100, input_length=max_len))\n",
    "    lstm_model.add(SpatialDropout1D(0.2))\n",
    "    lstm_model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))  # Reduced LSTM units\n",
    "    lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    lstm_model.fit(X_train_pad, y_train, epochs=2, batch_size=32, validation_data=(X_test_pad, y_test))  # Reduced epochs\n",
    "    lstm_model.save('lstm_model.h5')  # Save the trained model\n",
    "    print(\"LSTM model trained and saved to disk.\")\n",
    "\n",
    "# Calculate accuracy for LSTM\n",
    "lstm_pred = lstm_model.predict(X_test_pad)\n",
    "lstm_pred_binary = (lstm_pred > 0.5).astype(int)  # Convert probabilities to binary output\n",
    "lstm_accuracy = accuracy_score(y_test, lstm_pred_binary)\n",
    "print(f\"LSTM Model Accuracy: {lstm_accuracy:.2f}\")\n",
    "\n",
    "# Function to predict using Logistic Regression\n",
    "def predict_with_logistic(review_text):\n",
    "    review_tfidf = vectorizer.transform([review_text])\n",
    "    prediction = logistic_model.predict(review_tfidf)\n",
    "    label = 'Computer Generated' if prediction[0] == 1 else 'Original Review'\n",
    "    return label\n",
    "\n",
    "# Function to predict using LSTM\n",
    "def predict_with_lstm(review_text):\n",
    "    review_seq = tokenizer.texts_to_sequences([review_text])\n",
    "    review_pad = pad_sequences(review_seq, maxlen=max_len)\n",
    "    prediction = lstm_model.predict(review_pad)\n",
    "    label = 'Computer Generated' if prediction[0][0] > 0.5 else 'Original Review'\n",
    "    return label\n",
    "\n",
    "# Get user input\n",
    "user_review = input(\"Please enter a review to classify: \")\n",
    "\n",
    "# Make predictions with both models\n",
    "logistic_result = predict_with_logistic(user_review)\n",
    "lstm_result = predict_with_lstm(user_review)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Logistic Regression Prediction: {logistic_result}\")\n",
    "print(f\"LSTM Prediction: {lstm_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fe9c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
